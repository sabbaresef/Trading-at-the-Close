{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3901f65c",
   "metadata": {
    "papermill": {
     "duration": 0.006547,
     "end_time": "2023-11-05T11:07:43.391181",
     "exception": false,
     "start_time": "2023-11-05T11:07:43.384634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f75f035",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-05T11:07:43.408160Z",
     "iopub.status.busy": "2023-11-05T11:07:43.407641Z",
     "iopub.status.idle": "2023-11-05T11:07:54.165560Z",
     "shell.execute_reply": "2023-11-05T11:07:54.164495Z"
    },
    "papermill": {
     "duration": 10.76922,
     "end_time": "2023-11-05T11:07:54.168357",
     "exception": false,
     "start_time": "2023-11-05T11:07:43.399137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import tqdm\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "from collections import deque\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# To train a model to predict target\n",
    "training = True\n",
    "prepare_dataset = True\n",
    "\n",
    "# To save models and dataset\n",
    "save = False\n",
    "\n",
    "# Submission\n",
    "submission = True\n",
    "\n",
    "# To load submission models or used trained ones\n",
    "load_submission_model = False\n",
    "\n",
    "# Seed\n",
    "seed = 42\n",
    "\n",
    "# Verbosity\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984f408",
   "metadata": {
    "papermill": {
     "duration": 0.00626,
     "end_time": "2023-11-05T11:07:54.181128",
     "exception": false,
     "start_time": "2023-11-05T11:07:54.174868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9048036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T11:07:54.195829Z",
     "iopub.status.busy": "2023-11-05T11:07:54.195136Z",
     "iopub.status.idle": "2023-11-05T11:07:54.231837Z",
     "shell.execute_reply": "2023-11-05T11:07:54.230714Z"
    },
    "papermill": {
     "duration": 0.047261,
     "end_time": "2023-11-05T11:07:54.234535",
     "exception": false,
     "start_time": "2023-11-05T11:07:54.187274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data_from_file(path='/kaggle/input/optiver-trading-at-the-close/train.csv'):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def load_model(model_path, compile=False):\n",
    "    return tf.keras.models.load_model(model_path, compile=compile)\n",
    "\n",
    "\n",
    "def load_input_scaler(input_scaler_path='/kaggle/working/input_scaler.pkl'):\n",
    "    with open(input_scaler_path, \"rb\") as f:\n",
    "        scaler = pickle.load(f)\n",
    "    return scaler\n",
    "\n",
    "\n",
    "def fill_sequence(sequence, step, n_features):\n",
    "    # A function to fill sequence with the mask value  \n",
    "    new_sequence = copy.deepcopy(sequence)        \n",
    "    for i in range(step - len(sequence)):\n",
    "        new_sequence.append(np.array([mask_value]*(n_features)).reshape(1, -1))\n",
    "    return np.array(new_sequence)\n",
    "\n",
    "\n",
    "def fill_targets(targets, step, n_features):\n",
    "    # A function to fill sequence with the mask value  \n",
    "    new_targets = copy.deepcopy(targets)        \n",
    "    for i in range(step - len(targets)):\n",
    "        new_targets.append(0.0)\n",
    "    return np.array(new_targets)\n",
    "\n",
    "\n",
    "def features_engineering(df, runtime=False):   \n",
    "    if not runtime:\n",
    "        # Drop NaN values\n",
    "        df = df.dropna()\n",
    "        \n",
    "        # Drop unused columns\n",
    "        df = df.drop(columns=['far_price', 'near_price', 'row_id', 'time_id'])\n",
    "    else:\n",
    "        # Drop unused columns\n",
    "        df = df.drop(columns=['far_price', 'near_price'])\n",
    "                \n",
    "    df[\"bid_ask_imbalance\"] = df.eval(\"(bid_size - ask_size) / (bid_size + ask_size)\")    \n",
    "    df[\"ask_size_x_price\"] = df.eval(\"ask_size * ask_price\")\n",
    "    df[\"bid_size_x_price\"] = df.eval(\"bid_size * bid_price\")\n",
    "    df[\"complex_imbalance\"] = df.eval(\"(bid_size_x_price - ask_size_x_price) / (bid_size_x_price + ask_size_x_price)\")\n",
    "    df[\"prices_average\"] = df.eval('(reference_price + ask_price + bid_price + wap)/4')\n",
    "    \n",
    "    prices = ['reference_price', 'ask_price', 'bid_price', 'wap']\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_spread\"] = df.eval(f\"{c[0]}-{c[1]}\")\n",
    "        df[f\"{c[0]}_{c[1]}_urgency\"] = df[f\"{c[0]}_{c[1]}_spread\"] * df[\"bid_ask_imbalance\"]\n",
    "        df[f'{c[0]}_{c[1]}_imbalance'] = df.eval(f'({c[0]}-{c[1]})/({c[0]}+{c[1]})')\n",
    "        df[f'{c[0]}_x_{c[1]}'] = df.eval(f'{c[0]} * {c[1]}')\n",
    "    \n",
    "    # Drop spred columns\n",
    "    columns_to_keep = [col for col in df.columns if 'spread' not in col]\n",
    "    df = df[columns_to_keep]\n",
    "    \n",
    "    df = df.drop(columns=['ask_size_x_price', 'bid_size_x_price'])\n",
    "    \n",
    "     # Drop NaN values\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_data(df,\n",
    "                 mask_value=np.float32(999999999999.0),\n",
    "                 step=4,\n",
    "                 save=True,\n",
    "                 sequences_file_name='/kaggle/working/sequences.npy',\n",
    "                 targets_file_name='/kaggle/working/targets.npy',\n",
    "                 input_scaler_file_name='/kaggle/working/input_scaler.pkl'):\n",
    "     \n",
    "    # Features Engineering\n",
    "    df = features_engineering(df, runtime=False)\n",
    "\n",
    "    # Data standardization\n",
    "    features_to_remove_for_scaler = ['stock_id', 'date_id', 'target']\n",
    "    input_scaler = StandardScaler()\n",
    "    input_scaler.fit(df.drop(features_to_remove_for_scaler, axis=1, inplace=False))\n",
    "    \n",
    "    # Number of features in the final dataset\n",
    "    n_features = len(df.columns) - len(features_to_remove_for_scaler)\n",
    "    \n",
    "    # Group the data by stock_id and date_id\n",
    "    grouped = df.groupby(['stock_id', 'date_id'])\n",
    "    \n",
    "    # Lists to fill\n",
    "    data_sequences = []\n",
    "    targets_sequences = []\n",
    "    \n",
    "    # Loop over data\n",
    "    for group_name, group_data in tqdm.tqdm(grouped):\n",
    "        \n",
    "        # Get names\n",
    "        stock_id, date_id = group_name\n",
    "        \n",
    "        # Order data by 'seconds_in_bucket'\n",
    "        group_data = group_data.sort_values(by='seconds_in_bucket')\n",
    "        \n",
    "        # Deque to fill\n",
    "        sequence = deque(maxlen=step)\n",
    "        targets = deque(maxlen=step)\n",
    "                \n",
    "        # Iterate over a grup of data\n",
    "        for i in range(len(group_data)):\n",
    "            \n",
    "            # Get the row\n",
    "            row = group_data.iloc[i]\n",
    "               \n",
    "            # Get the target\n",
    "            target = row['target']\n",
    "            \n",
    "            # Standardize the data\n",
    "            data = row.drop(features_to_remove_for_scaler).to_numpy().reshape(1, -1)\n",
    "            standardized_data = input_scaler.transform(data)\n",
    "            \n",
    "            # Fill deque\n",
    "            sequence.append(standardized_data)\n",
    "            targets.append(target)\n",
    "            \n",
    "            # Fill up to steps\n",
    "            new_sequence = fill_sequence(sequence, step, n_features)\n",
    "            new_targets = fill_targets(targets, step, n_features)\n",
    "              \n",
    "            # Fill data sequence list\n",
    "            data_sequences.append(np.squeeze(new_sequence, axis=1))\n",
    "            \n",
    "            # Fill targets list\n",
    "            targets_sequences.append(new_targets)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    data_sequences = np.array(data_sequences, dtype='float32')\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    targets_sequences = np.array(targets_sequences, dtype='float32')\n",
    "    \n",
    "    if save:\n",
    "        print(f\"Saving sequences to {sequences_file_name}\")\n",
    "        np.save(sequences_file_name, data_sequences, allow_pickle=True)\n",
    "    \n",
    "        print(f\"Saving targets to {targets_file_name}\")\n",
    "        np.save(targets_file_name, targets_sequences, allow_pickle=True)\n",
    "        \n",
    "        print(f\"Saving the input scaler {input_scaler_file_name}\")\n",
    "        pickle.dump(input_scaler, open(input_scaler_file_name, 'wb'))\n",
    "    \n",
    "    # Run the garbage collector\n",
    "    gc.collect()\n",
    "    \n",
    "    return data_sequences, targets_sequences, input_scaler\n",
    "\n",
    "\n",
    "def load_dataset_and_scaler(sequences_file_name='/kaggle/working/sequences.npy',\n",
    "                            targets_file_name='/kaggle/working/targets.npy',\n",
    "                            input_scaler_file_name='/kaggle/working/input_scaler.pkl'):\n",
    "    # Sequences\n",
    "    with open(sequences_file_name, \"rb\") as f:\n",
    "        sequences = np.load(f, allow_pickle=True)\n",
    "         \n",
    "    # Targets\n",
    "    with open(targets_file_name, \"rb\") as f:\n",
    "        targets = np.load(f, allow_pickle=True)\n",
    "        \n",
    "    # Input scaler\n",
    "    with open(input_scaler_file_name, \"rb\") as f:\n",
    "        input_scaler = pickle.load(f)\n",
    "            \n",
    "    return sequences, targets, input_scaler\n",
    "\n",
    "\n",
    "\n",
    "class CleanCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            K.clear_session()\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "def Ranger(sync_period=6,\n",
    "           slow_step_size=0.5,\n",
    "           learning_rate=0.001,\n",
    "           beta_1=0.9,\n",
    "           beta_2=0.999,\n",
    "           epsilon=1e-7,\n",
    "           weight_decay=0.,\n",
    "           amsgrad=False,\n",
    "           sma_threshold=5.0,\n",
    "           total_steps=0,\n",
    "           warmup_proportion=0.1,\n",
    "           min_lr=0.,\n",
    "           name=\"Ranger\"):\n",
    "    \"\"\"\n",
    "        function returning a tf.keras.optimizers.Optimizer object\n",
    "        returned optimizer is a Ranger optimizer\n",
    "        Ranger is an optimizer combining RAdam (https://arxiv.org/abs/1908.03265) and Lookahead (https://arxiv.org/abs/1907.0861)\n",
    "        returned optimizer can be fed into the model.compile method of a tf.keras model as an optimizer\n",
    "        ...\n",
    "        Attributes\n",
    "        ----------\n",
    "        learning_rate : float\n",
    "            step size to take for RAdam optimizer (depending on gradient)\n",
    "        beta_1 : float\n",
    "            parameter that specifies the exponentially moving average length for momentum (0<=beta_1<=1)\n",
    "        beta_2 : float\n",
    "            parameter that specifies the exponentially moving average length for variance (0<=beta_2<=1)\n",
    "        epsilon : float\n",
    "            small number to cause stability for variance division\n",
    "        weight_decay : float\n",
    "            number with which the weights of the model are multiplied each iteration (0<=weight_decay<=1)\n",
    "        amsgrad : bool\n",
    "            parameter that specifies whether to use amsgrad version of Adam (https://arxiv.org/abs/1904.03590)\n",
    "        total_steps : int\n",
    "            total number of training steps\n",
    "        warmup_proportion : float\n",
    "            the proportion of updated over which the learning rate is increased from min learning rate to learning rate (0<=warmup_proportion<=1)\n",
    "        min_lr : float\n",
    "            learning rate at which the optimizer starts\n",
    "        k : int\n",
    "            parameter that specifies after how many steps the lookahead step backwards should be applied\n",
    "        alpha : float\n",
    "            parameter that specifies how much in the direction of the fast weights should be moved (0<=alpha<=1)\n",
    "    \"\"\"\n",
    "    # create RAdam optimizer\n",
    "    inner = tfa.optimizers.RectifiedAdam(learning_rate, beta_1, beta_2, epsilon, weight_decay, amsgrad, sma_threshold, total_steps, warmup_proportion, min_lr, name)\n",
    "    # feed RAdam optimizer into lookahead operation\n",
    "    optim = tfa.optimizers.Lookahead(inner, sync_period, slow_step_size, name)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a922b",
   "metadata": {
    "papermill": {
     "duration": 0.006188,
     "end_time": "2023-11-05T11:07:54.247106",
     "exception": false,
     "start_time": "2023-11-05T11:07:54.240918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e895bd5",
   "metadata": {
    "papermill": {
     "duration": 0.006478,
     "end_time": "2023-11-05T11:07:54.259778",
     "exception": false,
     "start_time": "2023-11-05T11:07:54.253300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e2fe891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T11:07:54.274531Z",
     "iopub.status.busy": "2023-11-05T11:07:54.274111Z",
     "iopub.status.idle": "2023-11-05T11:07:54.282014Z",
     "shell.execute_reply": "2023-11-05T11:07:54.280645Z"
    },
    "papermill": {
     "duration": 0.018154,
     "end_time": "2023-11-05T11:07:54.284272",
     "exception": false,
     "start_time": "2023-11-05T11:07:54.266118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mask value\n",
    "mask_value = np.float32(999999999999.0)\n",
    "\n",
    "# Constants for the dataset\n",
    "step = 4\n",
    "shuffle_factor = 1 # 1\n",
    "take_factor = 1 # 1 \n",
    "\n",
    "# Constants for the model\n",
    "units_type = \"gru\"\n",
    "epochs = 24 # 256\n",
    "batch_size = 512\n",
    "n_units = 12\n",
    "optimizer = \"adamw\"\n",
    "weight_decay = 0.025  # 0.015 \n",
    "loss = \"mae\"\n",
    "learning_rate = 0.001 # 0.005 \n",
    "dropout = 0.25\n",
    "\n",
    "# Callbacks to use\n",
    "use_early_stopping = False # True\n",
    "use_lr_reduce = False # True\n",
    "min_delta = 0.0001\n",
    "early_stopping_patience = 6\n",
    "reduce_lr_patience = 4\n",
    "\n",
    "use_lr_schedule = not use_lr_reduce and True # False\n",
    "lr_schedule_step = 6\n",
    "\n",
    "\n",
    "if save:\n",
    "    # Experiments number\n",
    "    experiment_number = 16\n",
    "    \n",
    "    # Path to save models\n",
    "    save_model_path = f'/kaggle/working/Model_{experiment_number}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f453e",
   "metadata": {
    "papermill": {
     "duration": 0.006159,
     "end_time": "2023-11-05T11:07:54.296893",
     "exception": false,
     "start_time": "2023-11-05T11:07:54.290734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b8dfc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T11:07:54.311841Z",
     "iopub.status.busy": "2023-11-05T11:07:54.311245Z",
     "iopub.status.idle": "2023-11-05T11:42:22.459610Z",
     "shell.execute_reply": "2023-11-05T11:42:22.458492Z"
    },
    "papermill": {
     "duration": 2068.159196,
     "end_time": "2023-11-05T11:42:22.462410",
     "exception": false,
     "start_time": "2023-11-05T11:07:54.303214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95212/95212 [33:46<00:00, 46.99it/s]\n"
     ]
    }
   ],
   "source": [
    "if training:\n",
    "    if prepare_dataset:\n",
    "        # Loat data from file\n",
    "        df = load_data_from_file()\n",
    "\n",
    "        # Prepare the dataset\n",
    "        sequences, targets, input_scaler = prepare_data(df, mask_value=mask_value, step=step, save=save)\n",
    "    else:\n",
    "        # If the datase is already available, load it\n",
    "        sequences, targets, input_scaler = load_dataset_and_scaler()\n",
    "    \n",
    "    # Add a dimension\n",
    "    targets = targets[:, :, None]\n",
    "    \n",
    "    # Split the dataset\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(sequences, targets, train_size=0.7, random_state=seed)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=seed)\n",
    "    \n",
    "    # Dataset Pipeline\n",
    "    training_set = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    training_set = training_set.shuffle(int(X_train.shape[0] * shuffle_factor))\n",
    "    training_set = training_set.take(int(X_train.shape[0] * take_factor))\n",
    "    training_set = training_set.batch(batch_size, drop_remainder=True)\n",
    "    training_set = training_set.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    validation_set = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    validation_set = validation_set.batch(batch_size, drop_remainder=True)\n",
    "    validation_set = validation_set.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e43de1",
   "metadata": {
    "papermill": {
     "duration": 1.752383,
     "end_time": "2023-11-05T11:42:25.976845",
     "exception": false,
     "start_time": "2023-11-05T11:42:24.224462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14763738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T11:42:29.417256Z",
     "iopub.status.busy": "2023-11-05T11:42:29.416440Z",
     "iopub.status.idle": "2023-11-05T11:57:33.164060Z",
     "shell.execute_reply": "2023-11-05T11:57:33.162841Z"
    },
    "papermill": {
     "duration": 905.516923,
     "end_time": "2023-11-05T11:57:33.166549",
     "exception": false,
     "start_time": "2023-11-05T11:42:27.649626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4, 31)]      0           []                               \n",
      "                                                                                                  \n",
      " masking (Masking)              (None, 4, 31)        0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " gru (GRU)                      (None, 4, 12)        1620        ['masking[0][0]']                \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 4, 12)       156         ['gru[0][0]']                    \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 4, 12)        0           ['gru[0][0]',                    \n",
      "                                                                  'time_distributed[0][0]']       \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 4, 1)        13          ['multiply[0][0]']               \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,789\n",
      "Trainable params: 1,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Model Fit\n",
      "Epoch 1/24\n",
      "3204/3204 [==============================] - 37s 10ms/step - loss: 5.5997 - val_loss: 5.5731 - lr: 0.0010\n",
      "Epoch 2/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5821 - val_loss: 5.5652 - lr: 0.0010\n",
      "Epoch 3/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5766 - val_loss: 5.5597 - lr: 0.0010\n",
      "Epoch 4/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5728 - val_loss: 5.5552 - lr: 0.0010\n",
      "Epoch 5/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5702 - val_loss: 5.5532 - lr: 0.0010\n",
      "Epoch 6/24\n",
      "3204/3204 [==============================] - 34s 9ms/step - loss: 5.5683 - val_loss: 5.5510 - lr: 0.0010\n",
      "Epoch 7/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5665 - val_loss: 5.5494 - lr: 5.0000e-04\n",
      "Epoch 8/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5659 - val_loss: 5.5491 - lr: 5.0000e-04\n",
      "Epoch 9/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5657 - val_loss: 5.5487 - lr: 5.0000e-04\n",
      "Epoch 10/24\n",
      "3204/3204 [==============================] - 34s 9ms/step - loss: 5.5649 - val_loss: 5.5478 - lr: 5.0000e-04\n",
      "Epoch 11/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5644 - val_loss: 5.5474 - lr: 5.0000e-04\n",
      "Epoch 12/24\n",
      "3204/3204 [==============================] - 35s 9ms/step - loss: 5.5642 - val_loss: 5.5467 - lr: 5.0000e-04\n",
      "Epoch 13/24\n",
      "3204/3204 [==============================] - 34s 9ms/step - loss: 5.5637 - val_loss: 5.5463 - lr: 2.5000e-04\n",
      "Epoch 14/24\n",
      "3204/3204 [==============================] - 35s 9ms/step - loss: 5.5633 - val_loss: 5.5468 - lr: 2.5000e-04\n",
      "Epoch 15/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5629 - val_loss: 5.5460 - lr: 2.5000e-04\n",
      "Epoch 16/24\n",
      "3204/3204 [==============================] - 35s 9ms/step - loss: 5.5630 - val_loss: 5.5456 - lr: 2.5000e-04\n",
      "Epoch 17/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5627 - val_loss: 5.5456 - lr: 2.5000e-04\n",
      "Epoch 18/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5625 - val_loss: 5.5452 - lr: 2.5000e-04\n",
      "Epoch 19/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5625 - val_loss: 5.5451 - lr: 1.2500e-04\n",
      "Epoch 20/24\n",
      "3204/3204 [==============================] - 36s 10ms/step - loss: 5.5619 - val_loss: 5.5448 - lr: 1.2500e-04\n",
      "Epoch 21/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5618 - val_loss: 5.5452 - lr: 1.2500e-04\n",
      "Epoch 22/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5620 - val_loss: 5.5447 - lr: 1.2500e-04\n",
      "Epoch 23/24\n",
      "3204/3204 [==============================] - 36s 10ms/step - loss: 5.5617 - val_loss: 5.5447 - lr: 1.2500e-04\n",
      "Epoch 24/24\n",
      "3204/3204 [==============================] - 35s 10ms/step - loss: 5.5617 - val_loss: 5.5448 - lr: 1.2500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if training:\n",
    "    # Optimizer\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"rmsprop\":\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == \"adamw\":\n",
    "        optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer == \"ranger\":\n",
    "        optimizer = Ranger(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [] # To fill\n",
    "    \n",
    "    clean = CleanCallback()\n",
    "    callbacks.append(clean)\n",
    "    \n",
    "    if use_early_stopping:\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=min_delta,\n",
    "            patience=early_stopping_patience,\n",
    "            verbose=verbose,\n",
    "            restore_best_weights=True)\n",
    "        callbacks.append(early_stopping)\n",
    "\n",
    "    if use_lr_reduce:\n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',                                \n",
    "            min_delta=min_delta,                            \n",
    "            factor=0.5,\n",
    "            patience=reduce_lr_patience,\n",
    "            verbose=verbose)\n",
    "        callbacks.append(reduce_lr)\n",
    "        \n",
    "    if use_lr_schedule:\n",
    "        def scheduler(epoch, lr):\n",
    "            if epoch == 0:\n",
    "                return lr\n",
    "\n",
    "            if epoch % lr_schedule_step == 0:\n",
    "                return lr / 2\n",
    "            else:\n",
    "                return lr\n",
    "            \n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "        callbacks.append(lr_schedule)\n",
    "    \n",
    "    \n",
    "    # Model definition\n",
    "    model_input = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    mask = tf.keras.layers.Masking(mask_value=mask_value)(model_input)\n",
    "    \n",
    "    if units_type == \"gru\":\n",
    "        hidden = tf.keras.layers.GRU(n_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout)(mask)\n",
    "    else:\n",
    "        hidden = tf.keras.layers.LSTM(n_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout)(mask)\n",
    "    \n",
    "    attention = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=n_units, activation='softmax'))(hidden)\n",
    "    hidden = tf.keras.layers.Multiply()([hidden, attention])\n",
    "    \n",
    "    model_output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=1, activation='linear'))(hidden)\n",
    "    model = tf.keras.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "    # Model summary\n",
    "    print(\"Model Summary\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Model compile\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=None)\n",
    "    \n",
    "    # Model Fit\n",
    "    print(\"\\nModel Fit\")\n",
    "    model.fit(\n",
    "            training_set,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=validation_set)\n",
    "    \n",
    "    if save:\n",
    "        # Save the model to the path\n",
    "        print(f\"\\nModel Saving at: {save_model_path}\")\n",
    "        model.save(save_model_path, save_format='tf')\n",
    "\n",
    "        # Evaluate the model \n",
    "        test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "        test_set = test_set.batch(batch_size, drop_remainder=True)\n",
    "        test_set = test_set.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        print(\"\\nModel Evaluation\")\n",
    "        model.evaluate(test_set, verbose=verbose)\n",
    "\n",
    "        print(\"\\nModel Prediction\")\n",
    "        results = model.predict(test_set, verbose=verbose)\n",
    "        \n",
    "# Clear\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6055d4e",
   "metadata": {
    "papermill": {
     "duration": 2.936439,
     "end_time": "2023-11-05T11:57:39.102319",
     "exception": false,
     "start_time": "2023-11-05T11:57:36.165880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3491d87",
   "metadata": {
    "papermill": {
     "duration": 2.999536,
     "end_time": "2023-11-05T11:57:44.906664",
     "exception": false,
     "start_time": "2023-11-05T11:57:41.907128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb04ad0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T11:57:50.729928Z",
     "iopub.status.busy": "2023-11-05T11:57:50.729542Z",
     "iopub.status.idle": "2023-11-05T11:57:50.745532Z",
     "shell.execute_reply": "2023-11-05T11:57:50.744553Z"
    },
    "papermill": {
     "duration": 3.03787,
     "end_time": "2023-11-05T11:57:50.748017",
     "exception": false,
     "start_time": "2023-11-05T11:57:47.710147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zero_sum(prices, volumes):  \n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices)/np.sum(std_error)\n",
    "    out = prices-std_error*step\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_submission_data(df,\n",
    "                            data_history,\n",
    "                            input_scaler,\n",
    "                            step=4):\n",
    "    \n",
    "    # Features Engineering\n",
    "    df = features_engineering(df, runtime=True)\n",
    "                    \n",
    "    # Constants\n",
    "    features_to_remove_for_scaler = ['stock_id', 'date_id', 'row_id']\n",
    "    \n",
    "    # Number of features in the final dataset\n",
    "    n_features = len(df.columns) - len(features_to_remove_for_scaler)\n",
    "    \n",
    "    # Group the data by stock_id and date_id\n",
    "    grouped = df.groupby(['stock_id', 'date_id'])\n",
    "    \n",
    "    # Loop over data\n",
    "    for group_name, group_data in grouped:\n",
    "        \n",
    "        # Get names\n",
    "        stock_id, date_id = group_name\n",
    "\n",
    "        # Access the data\n",
    "        row = group_data.iloc[0]\n",
    "\n",
    "        if stock_id not in data_history:            \n",
    "            # Init the deque\n",
    "            data_deque = deque(maxlen=step)\n",
    "            \n",
    "            # Get row_id\n",
    "            row_id = row['row_id']\n",
    "                        \n",
    "            # Standardize the data\n",
    "            data = row.drop(features_to_remove_for_scaler).to_numpy().reshape(1, -1)\n",
    "            standardized_data = input_scaler.transform(data)\n",
    "            \n",
    "            # Fill the deque\n",
    "            data_deque.append(standardized_data)\n",
    "            \n",
    "            # Fill the history\n",
    "            d = {'date_id': date_id, 'data_deque': data_deque, 'row_id': row_id}\n",
    "            data_history.update({stock_id:d})\n",
    "        else:\n",
    "            # Read the dict from the history to update\n",
    "            d = data_history[stock_id]\n",
    "            \n",
    "            if data_history[stock_id]['date_id'] != date_id:                \n",
    "                # Init the deque\n",
    "                data_deque = deque(maxlen=step)\n",
    "            else:                                \n",
    "                # Get th data deque\n",
    "                data_deque = d['data_deque']\n",
    "\n",
    "            # Get row_id\n",
    "            row_id = row['row_id']\n",
    "\n",
    "            # Standardize the data\n",
    "            data = row.drop(features_to_remove_for_scaler).to_numpy().reshape(1, -1)\n",
    "            standardized_data = input_scaler.transform(data)\n",
    "\n",
    "            # Fill the deque\n",
    "            data_deque.append(standardized_data)\n",
    "\n",
    "            # Update the history               \n",
    "            d.update({'date_id': date_id})\n",
    "            d.update({'data_deque': data_deque})\n",
    "            d.update({'row_id': row_id})\n",
    "            data_history.update({stock_id:d})\n",
    "            \n",
    "    return data_history, n_features\n",
    "\n",
    "\n",
    "def search_dict_in_history(data_history, row_id):\n",
    "    for key, value in data_history.items():\n",
    "        if value['row_id'] == row_id:\n",
    "            return key, value\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe477eb",
   "metadata": {
    "papermill": {
     "duration": 2.984024,
     "end_time": "2023-11-05T11:57:56.513424",
     "exception": false,
     "start_time": "2023-11-05T11:57:53.529400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c510fa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T11:58:02.298192Z",
     "iopub.status.busy": "2023-11-05T11:58:02.297542Z",
     "iopub.status.idle": "2023-11-05T11:58:02.303576Z",
     "shell.execute_reply": "2023-11-05T11:58:02.302525Z"
    },
    "papermill": {
     "duration": 2.981112,
     "end_time": "2023-11-05T11:58:02.305925",
     "exception": false,
     "start_time": "2023-11-05T11:57:59.324813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if submission and load_submission_model: \n",
    "    # Mask value\n",
    "    mask_value = np.float32(999999999999.0)\n",
    "\n",
    "    # Step value\n",
    "    step = 4\n",
    "    \n",
    "    # Experiments number\n",
    "    experiment_number = 1\n",
    "    \n",
    "    # Path\n",
    "    save_model_path = f'/kaggle/working/Model_{experiment_number}'\n",
    "    \n",
    "    # Load the model\n",
    "    model = load_model(save_model_path)\n",
    "\n",
    "    # Load the scaler for standardization\n",
    "    input_scaler = load_input_scaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2df2c8",
   "metadata": {
    "papermill": {
     "duration": 2.994887,
     "end_time": "2023-11-05T11:58:08.163726",
     "exception": false,
     "start_time": "2023-11-05T11:58:05.168839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "195ff2aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T11:58:13.955807Z",
     "iopub.status.busy": "2023-11-05T11:58:13.955420Z",
     "iopub.status.idle": "2023-11-05T11:59:57.695688Z",
     "shell.execute_reply": "2023-11-05T11:59:57.694513Z"
    },
    "papermill": {
     "duration": 106.723708,
     "end_time": "2023-11-05T11:59:57.698688",
     "exception": false,
     "start_time": "2023-11-05T11:58:10.974980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    }
   ],
   "source": [
    "if submission:\n",
    "    # Init\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    \n",
    "    # Init an empty history\n",
    "    data_history = {}\n",
    "\n",
    "    # Loop over data\n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:    \n",
    "\n",
    "        # Update the history\n",
    "        data_history, n_features = prepare_submission_data(test, data_history, input_scaler, step)  \n",
    "\n",
    "        # Get stock ids to consider for prediction\n",
    "        row_ids_to_consider = sample_prediction['row_id']\n",
    "\n",
    "        # List to fill\n",
    "        input_data = []\n",
    "        output_index_to_select = []\n",
    "\n",
    "        # Loop over prediction data\n",
    "        for row_id in row_ids_to_consider:               \n",
    "            # Get the dict on the basi of row_id\n",
    "            stock_id, d = search_dict_in_history(data_history, row_id)\n",
    "\n",
    "            # Get the data deque from the dict\n",
    "            data_deque = d['data_deque']\n",
    "            \n",
    "            # Add the output index to select\n",
    "            output_index_to_select.append(len(data_deque) - 1)\n",
    "\n",
    "            # Fill the data_deque to step size\n",
    "            data_deque = fill_sequence(data_deque, step, n_features)        \n",
    "\n",
    "            # Remove the empty dimension\n",
    "            data_deque = np.squeeze(data_deque, axis=1)\n",
    "            \n",
    "            # Fill the list\n",
    "            input_data.append(data_deque)\n",
    "        \n",
    "        # Data pipeline\n",
    "        input_data = tf.data.Dataset.from_tensor_slices(input_data)\n",
    "        input_data = input_data.batch(256, drop_remainder=False)\n",
    "        \n",
    "        # Run the model inference\n",
    "        results = model.predict(input_data, verbose=0)\n",
    "                \n",
    "        selected_results = []\n",
    "        for res, i in zip(results, output_index_to_select):\n",
    "            selected_results.append(res[i])\n",
    "        \n",
    "        # Concert to numpy array\n",
    "        results = np.array(selected_results)\n",
    "        results = np.squeeze(results, axis=-1)\n",
    "        \n",
    "        # Assign the predictions (.copy)\n",
    "        sample_prediction['target'] = zero_sum(results, test['bid_size'] + test['ask_size'])\n",
    "                    \n",
    "        # Predict\n",
    "        env.predict(sample_prediction)\n",
    "        \n",
    "        # Clear\n",
    "        K.clear_session()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f7362",
   "metadata": {
    "papermill": {
     "duration": 3.037439,
     "end_time": "2023-11-05T12:00:03.614239",
     "exception": false,
     "start_time": "2023-11-05T12:00:00.576800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f6c11bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T12:00:09.570453Z",
     "iopub.status.busy": "2023-11-05T12:00:09.570059Z",
     "iopub.status.idle": "2023-11-05T12:00:09.576370Z",
     "shell.execute_reply": "2023-11-05T12:00:09.575351Z"
    },
    "papermill": {
     "duration": 3.105203,
     "end_time": "2023-11-05T12:00:09.578513",
     "exception": false,
     "start_time": "2023-11-05T12:00:06.473310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if submission and load_submission_model:\n",
    "    file_path = '/kaggle/working/submission.csv'\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        timestamp = os.path.getctime(file_path)\n",
    "        formatted_time = time.ctime(timestamp)\n",
    "\n",
    "    current_timestamp = time.time()\n",
    "    formatted_current_time = time.ctime(current_timestamp)\n",
    "\n",
    "    print(\"Check submission\")\n",
    "    print(formatted_time)\n",
    "    print(formatted_current_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3155.174768,
   "end_time": "2023-11-05T12:00:14.990401",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-05T11:07:39.815633",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
